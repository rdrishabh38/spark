services:
  # -------------------------------------------------------
  # 1. MINIO (S3 Emulation)
  # -------------------------------------------------------
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
    # Ephemeral Storage: Data is lost on 'docker compose down'
    command: server /data --console-address ":9001"

  # -------------------------------------------------------
  # 1.5 MINIO INIT (Automated Bucket Creation)
  # -------------------------------------------------------
  # This container starts, creates the bucket, and dies.
  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (mc alias set myminio http://minio:9000 admin password); do echo '...waiting for minio...'; sleep 1; done;
      mc mb --ignore-existing myminio/test-bucket;
      mc mb --ignore-existing myminio/delta-lake;
      echo 'âœ… Buckets created';
      exit 0;
      "

  # -------------------------------------------------------
  # 2. SPARK CLUSTER
  # -------------------------------------------------------
  spark-master:
    image: spark:3.5.3-scala2.12-java17-python3-ubuntu
    container_name: spark-master
    # FIX: Run as root to allow writing to bind mounts
    user: root
    environment:
      - SPARK_MASTER_HOST=spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    volumes:
       - ./work:/opt/spark/work-dir

  spark-worker-1:
    image: spark:3.5.3-scala2.12-java17-python3-ubuntu
    container_name: spark-worker-1
    # FIX: Run as root to allow writing to bind mounts
    user: root
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
       - ./work:/opt/spark/work-dir

  spark-worker-2:
    image: spark:3.5.3-scala2.12-java17-python3-ubuntu
    container_name: spark-worker-2
    # FIX: Run as root to allow writing to bind mounts
    user: root
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
       - ./work:/opt/spark/work-dir

  # -------------------------------------------------------
  # 3. SPARK CLIENT (Edge Node)
  # -------------------------------------------------------
  spark-client:
    build:
      context: .
      dockerfile: Dockerfile.client
    container_name: spark-client
    # FIX: Run as root to allow writing to bind mounts
    user: root
    ports:
      - "8888:8888"
      - "4040:4040"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      # UPDATE: We added hadoop-aws and aws-java-sdk-bundle to support S3
      - PYSPARK_SUBMIT_ARGS=--packages io.delta:delta-spark_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell
    depends_on:
      - spark-master
      - minio
    command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''
    volumes:
       - ./work:/opt/spark/work-dir