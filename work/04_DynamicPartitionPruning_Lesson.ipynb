{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80253edb-26e6-4276-bce8-4e8926dc652b",
   "metadata": {},
   "source": [
    "This module covers Dynamic Partition Pruning (DPP), one of the most powerful optimizations in Spark 3.0 for Data Warehousing (Star Schema) workloads.\n",
    "\n",
    "The Concept: \"The Telepathic Scan\"\n",
    "In Spark 2.4, if you joined a massive Fact Table (partitioned by Date) with a small Dimension Table (filtered to \"Yesterday\"), Spark often scanned ALL partitions of the Fact Table because it didn't know which dates resulted from the join until it was too late.\n",
    "\n",
    "In Spark 3.x, DPP allows the engine to:\n",
    "\n",
    "Run the filter on the Dimension table first.\n",
    "\n",
    "Take the resulting keys (e.g., specific dates).\n",
    "\n",
    "Inject those keys directly into the Fact Table scanner.\n",
    "\n",
    "Skip reading files that don't match the join keys.\n",
    "\n",
    "Lab 1.2: Dynamic Partition Pruning\n",
    "Objective: Prove that filtering a small table restricts the file scanning of a large joined table.\n",
    "\n",
    "Step 1: Setup & Data Generation\n",
    "We need a Partitioned Fact Table (Sales) and a Dimension Table (Stores). Run this cell to generate the data on MinIO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0f239-b11a-417e-8cf1-a0510561cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-34904bf6-e91b-4687-aa10-c4ba699dc4b3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 172ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-34904bf6-e91b-4687-aa10-c4ba699dc4b3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/4ms)\n",
      "25/11/29 22:56:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Started!\n",
      "ðŸ‘‰ Go open http://localhost:4040 in a new tab NOW.\n",
      "   (It will show 'No active stages', which is correct)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initialize (Run Once)\n",
    "# As soon as you run this, http://localhost:4040 will become live.\n",
    " \n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, sum\n",
    "\n",
    "# Initialize Spark with MinIO/S3 capabilities\n",
    "# We use a 1GB limit to ensure we don't crash the container\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter_DPP_Lesson\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.cores.max\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce log noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"âœ… Spark Session Started!\")\n",
    "print(f\"ðŸ‘‰ Go open http://localhost:4040 in a new tab NOW.\")\n",
    "print(f\"   (It will show 'No active stages', which is correct)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96fd5ff9-4acc-4b17-9773-dcede8531b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating Data for DPP...\n",
      "\n",
      "\n",
      "Dimensional data\n",
      "+---------+----------+\n",
      "|region_id|store_code|\n",
      "+---------+----------+\n",
      "|0        |0         |\n",
      "|1        |10        |\n",
      "|2        |20        |\n",
      "|3        |30        |\n",
      "|4        |40        |\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fact data\n",
      "+------+------------------+---------+\n",
      "|txn_id|amount            |region_id|\n",
      "+------+------------------+---------+\n",
      "|0     |89.91179044749333 |0        |\n",
      "|1     |36.13195951313584 |1        |\n",
      "|2     |62.93333165418805 |2        |\n",
      "|3     |30.738846507147997|3        |\n",
      "|4     |42.99228827796312 |4        |\n",
      "+------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "ðŸ’¾ Writing Fact Table to s3a://test-bucket/fact_sales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Generated.\n"
     ]
    }
   ],
   "source": [
    "#Cell 2: Generate Data (Run Once)\n",
    "# This writes to S3. You will see jobs appear in the UI while this runs.\n",
    "\n",
    "\n",
    "# Paths\n",
    "FACT_PATH = \"s3a://test-bucket/fact_sales\"\n",
    "DIM_PATH = \"s3a://test-bucket/dim_stores\"\n",
    "\n",
    "print(\"ðŸš€ Generating Data for DPP...\")\n",
    "\n",
    "# 1. Create Dimension Table (Regions)\n",
    "# 5 Regions: 0, 1, 2, 3, 4\n",
    "df_dim = spark.range(0, 5).select(col(\"id\").alias(\"region_id\"), (col(\"id\") * 10).alias(\"store_code\"))\n",
    "\n",
    "print(\"\\n\\nDimensional data\")\n",
    "df_dim.show(truncate=False)\n",
    "df_dim.write.format(\"delta\").mode(\"overwrite\").save(DIM_PATH)\n",
    "\n",
    "# 2. Create Fact Table (Sales) - Partitioned by 'region_id'\n",
    "# 5 Million rows distributed across 5 regions.\n",
    "df_fact = spark.range(0, 5000000).select(\n",
    "    col(\"id\").alias(\"txn_id\"), \n",
    "    (rand() * 100).alias(\"amount\"),\n",
    "    (col(\"id\") % 5).alias(\"region_id\") \n",
    ")\n",
    "\n",
    "print(\"\\n\\nFact data\")\n",
    "df_fact.show(5, truncate=False)\n",
    "\n",
    "print(f\"ðŸ’¾ Writing Fact Table to {FACT_PATH}...\")\n",
    "df_fact.write.format(\"delta\").partitionBy(\"region_id\").mode(\"overwrite\").save(FACT_PATH)\n",
    "\n",
    "print(\"âœ… Data Generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fac15a-f4fe-4f34-aa2b-c5d6a48a9249",
   "metadata": {},
   "source": [
    "### Observation: Data is generated in MiniO bucket\n",
    "\n",
    "\n",
    "## Fact Sales Data\n",
    "![Skew Stage Graph](screenshots/03_DPP_fact_sales_data.jpg)\n",
    "\n",
    "\n",
    "## Dimension Region Data\n",
    "![Skew Stage Graph](screenshots/04_DPP_dimension_region_data.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c026a7e-8ba2-4f5b-9339-aa6b02802cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: The Test (DPP Enabled vs Disabled)\n",
    "We will join Sales (Fact) with Stores (Dim). We filter Stores to only look for Region 0.\n",
    "\n",
    "Without DPP: Spark should scan all 5,000,000 rows (all 5 regions) from the Fact table, then filter them out during the join.\n",
    "\n",
    "With DPP: Spark should only scan 1,000,000 rows (Region 0) from the Fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51e6af3-33d9-46e6-a7ab-c6e4b5b28160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING WITH DPP DISABLED ---\n",
      "+---------+-------------------+\n",
      "|region_id|sum(amount)        |\n",
      "+---------+-------------------+\n",
      "|0        |5.002179944479497E7|\n",
      "+---------+-------------------+\n",
      "\n",
      "â±ï¸ Duration: 1.58 seconds\n",
      "ðŸ‘‰ Check UI -> SQL Tab -> Click Query.\n",
      "   Look at the 'Scan delta' box. It should read ALL files (approx 50MB).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Test 1 - DPP DISABLED (The \"Slow\" Way)\n",
    "# Run this, then look at the SQL tab in the UI.\n",
    "\n",
    "print(\"--- RUNNING WITH DPP DISABLED ---\")\n",
    "\n",
    "# Disable DPP\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"false\")\n",
    "spark.catalog.clearCache() # Clear cache to force S3 read\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read Tables\n",
    "fact = spark.read.format(\"delta\").load(FACT_PATH)\n",
    "dim = spark.read.format(\"delta\").load(DIM_PATH)\n",
    "\n",
    "# QUERY: Join Fact + Dim, filtering only on Dim\n",
    "result = (\n",
    "    fact.join(dim, \"region_id\")\n",
    "    .filter(dim.store_code == 0) \n",
    "    .groupBy(dim.region_id)\n",
    "    .agg(sum(\"amount\"))\n",
    ")\n",
    "\n",
    "result.collect()\n",
    "result.show(truncate=False)\n",
    "\n",
    "print(f\"â±ï¸ Duration: {time.time() - start:.2f} seconds\")\n",
    "print(\"ðŸ‘‰ Check UI -> SQL Tab -> Click Query.\")\n",
    "print(\"   Look at the 'Scan parquet' box. It should read ALL files (approx 50MB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31ceaa-c866-4ea9-b2c9-25f5bc1f3d00",
   "metadata": {},
   "source": [
    "## DPP disabled reads all data - 10 files 57 MB\n",
    "![Skew Stage Graph](screenshots/05_DPP_disabled_read_all_rows.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d793b065-aadd-451a-bf2e-ca125b07562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING WITH DPP ENABLED ---\n",
      "+---------+--------------------+\n",
      "|region_id|sum(amount)         |\n",
      "+---------+--------------------+\n",
      "|0        |5.0021799444798514E7|\n",
      "+---------+--------------------+\n",
      "\n",
      "â±ï¸ Duration: 1.20 seconds\n",
      "ðŸ‘‰ Check UI -> SQL Tab -> Click the NEW Query.\n",
      "   Look at 'Scan delta'. It should read ONLY 1/5th of the files (approx 10MB).\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test 2 - DPP ENABLED (The \"Fast\" Way)\n",
    "# Run this immediately after. The session stays alive.\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- RUNNING WITH DPP ENABLED ---\")\n",
    "\n",
    "# Enable DPP\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fact = spark.read.format(\"delta\").load(FACT_PATH)\n",
    "dim = spark.read.format(\"delta\").load(DIM_PATH)\n",
    "\n",
    "# We filter on 'store_code', NOT the join key 'region_id'.\n",
    "# Spark CANNOT statically optimize this. It MUST use DPP.\n",
    "result = (\n",
    "    fact.join(dim, \"region_id\")\n",
    "    .filter(dim.store_code == 0) \n",
    "    .groupBy(dim.region_id)\n",
    "    .agg(sum(\"amount\"))\n",
    ")\n",
    "\n",
    "result.collect()\n",
    "result.show(truncate=False)\n",
    "\n",
    "print(f\"â±ï¸ Duration: {time.time() - start:.2f} seconds\")\n",
    "print(\"ðŸ‘‰ Check UI -> SQL Tab -> Click the NEW Query.\")\n",
    "print(\"   Look at 'Scan parquet'. It should read ONLY 1/5th of the files (approx 10MB).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdfac2-344c-4dbf-aee3-611d063291f3",
   "metadata": {},
   "source": [
    "## DPP disabled reads all data - 10 files 57 MB\n",
    "![Skew Stage Graph](screenshots/06_DPP_enabled_reads_less_rows.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38559d4-433f-4496-a869-692a11f9208c",
   "metadata": {},
   "source": [
    "How to verify in the UI\n",
    "Since you won't be rushing against a spark.stop() timer, you can take your time:\n",
    "\n",
    "Go to the SQL / DataFrame tab.\n",
    "\n",
    "You will see two completed queries.\n",
    "\n",
    "Click the Description of the first one (DPP Disabled).\n",
    "\n",
    "Find the Scan Delta node.\n",
    "\n",
    "Hover over it. Look for \"size of files read\". It should be the total size of your dataset.\n",
    "\n",
    "Click the Description of the second one (DPP Enabled).\n",
    "\n",
    "Find the Scan Delta node.\n",
    "\n",
    "The \"size of files read\" should be significantly smaller (exactly 20% of the total, since we selected 1 out of 5 regions).\n",
    "\n",
    "This confirms that Spark \"pushed\" the filter from the small table into the storage reader of the big table before reading the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dca26-7e8b-4686-8ad2-2093386f8ff4",
   "metadata": {},
   "source": [
    "# ðŸ§  Concept: How Dynamic Partition Pruning (DPP) Actually Works\n",
    "\n",
    "**The Question:** Is DPP based on *Static File Metadata* (like Parquet footers) or *Dynamic Engine Logic*?\n",
    "\n",
    "**The Answer:** It is **Dynamic Engine Logic**.\n",
    "\n",
    "While Parquet/Delta do store metrics (min/max values) in their footers for \"Data Skipping\" (skipping row groups *inside* a file), **Dynamic Partition Pruning (DPP)** operates at a higher level: it skips entire **Partition Directories** based on the results of a running query.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ The Mechanics: \"The Feedback Loop\"\n",
    "\n",
    "Usually, a database reads tables starting from the bottom (Scan) and moves up (Filter -> Join). DPP breaks this rule by creating a **Feedback Loop**.\n",
    "\n",
    "\n",
    "\n",
    "Here is the step-by-step execution flow:\n",
    "\n",
    "#### 1. The Dimension Side (The \"Scout\")\n",
    "Spark looks at your query plan and notices a specific pattern:\n",
    "* **Table A (Fact):** Partitioned by `region_id`.\n",
    "* **Table B (Dim):** Filtered (e.g., `store_code == 0`) and joined on `region_id`.\n",
    "\n",
    "Spark decides: *\"I need to run the Dimension side first.\"*\n",
    "It runs the filter on the Dimension table (`dim_stores`) and finds the matching keys.\n",
    "* *Result:* `region_id = [0]`\n",
    "\n",
    "#### 2. The Broadcast (The \"Phone Call\")\n",
    "This is the magic part. Instead of just sending that result to the Join operator, Spark wraps those keys (`[0]`) into a **BroadcastExchange**.\n",
    "It essentially \"broadcasts\" these valid keys to all executors.\n",
    "\n",
    "#### 3. The Injection (The \"Guard at the Door\")\n",
    "Spark takes that Broadcast variable and **injects it as a Filter** directly into the **Fact Table Scanner**.\n",
    "\n",
    "Before the Fact Table reader even looks at the file system, it checks this dynamic list.\n",
    "* *Scanner:* \"I see folders for Regions 0, 1, 2, 3, 4. Which ones should I open?\"\n",
    "* *DPP Filter:* \"I just got a call from the Dimension table. Only open **Region 0**.\"\n",
    "\n",
    "#### 4. The Pruned Scan\n",
    "The scanner now ignores the directories for Regions 1, 2, 3, and 4. It only reads the files in `region_id=0`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ†š Visualizing the Difference: Parquet Metrics vs. DPP\n",
    "\n",
    "It is crucial to distinguish between these two optimizations:\n",
    "\n",
    "| Feature | **Data Skipping (Parquet/Delta Metrics)** | **Dynamic Partition Pruning (DPP)** |\n",
    "| :--- | :--- | :--- |\n",
    "| **What it uses** | **Static Metadata:** Min/Max statistics stored in the file footer or `_delta_log`. | **Runtime Results:** The actual output of a query running on another table. |\n",
    "| **Scope** | **Intra-Partition:** \"Does this specific *file* contain ID 500?\" | **Inter-Partition:** \"Does this whole *folder* contain the Joined Keys?\" |\n",
    "| **Trigger** | `WHERE id = 500` (Direct Filter) | `JOIN ... ON ...` (Indirect Filter) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eac09-0ddd-424b-88af-c7b6e24d68e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
