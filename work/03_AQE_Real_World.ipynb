{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bcad84-7e89-441e-ac40-a9569fb124d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8670421e-f400-46cf-983b-25e0b1c492ff;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8670421e-f400-46cf-983b-25e0b1c492ff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/4ms)\n",
      "25/11/29 21:55:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & S3 Config\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Initialize Spark with S3 Support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter_AQE_Lesson\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Log Level to WARN to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Web UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abf9690-f5ec-4c90-a47a-3448f5e67881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 21:55:56 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prepare the DataFrames\n",
    "\n",
    "\n",
    "# 1. Load the Big Table (10M rows, skewed, fragmented)\n",
    "SALES_PATH = \"s3a://test-bucket/sales_data_skewed\"\n",
    "df_sales = spark.read.format(\"delta\").load(SALES_PATH)\n",
    "\n",
    "# 2. Create the Small Table (Dimension)\n",
    "countries = [(\"USA\", \"United States\"), (\"IND\", \"India\"), (\"UK\", \"United Kingdom\"), (\"Other\", \"Rest of World\")]\n",
    "df_countries = spark.createDataFrame(countries, [\"country_code\", \"country_name\"])\n",
    "\n",
    "print(\"Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126708b-80ff-47e2-b483-c81311b5ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: RUN 1 - AQE DISABLED (The \"Bad\" Way) Run this, then immediately check the Spark UI Stages tab.\n",
    "\n",
    "print(\"--- RUNNING WITH AQE DISABLED ---\")\n",
    "\n",
    "# Disable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "# Force defaults to simulate a 'static' plan\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "# Disable Broadcast to force a SortMergeJoin (which is sensitive to skew)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# The Heavy Join\n",
    "result = (\n",
    "    df_sales\n",
    "    .join(df_countries, \"country_code\")\n",
    "    .groupBy(\"country_name\")\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "# Trigger Action\n",
    "result.collect()\n",
    "\n",
    "print(f\"‚è±Ô∏è Duration (AQE OFF): {time.time() - start:.2f} seconds\")\n",
    "print(\"üëâ Go check http://localhost:4040 -> Stages. Look for '200' tasks and Skew.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfed25-ea6e-4e5d-b97c-471028fad949",
   "metadata": {},
   "source": [
    "### Observation: Data Skew\n",
    "Notice the tasks have 200 partitions even though we do not need 200 partitions.\n",
    "Since we defined 200 partitions, and the physical plan generated had 200 partitions, 200 partitions were spawned.\n",
    "Most of them did nothing since there was no data. You can open the spark UI and check yourself.\n",
    "\n",
    "\n",
    "The physical plan generated in this case was :\n",
    "\n",
    "```== Physical Plan ==\n",
    "* HashAggregate (14)\n",
    "+- Exchange (13)\n",
    "   +- * HashAggregate (12)\n",
    "      +- * Project (11)\n",
    "         +- * SortMergeJoin Inner (10)\n",
    "            :- * Sort (5)\n",
    "            :  +- Exchange (4)\n",
    "            :     +- * Filter (3)\n",
    "            :        +- * ColumnarToRow (2)\n",
    "            :           +- Scan parquet  (1)\n",
    "            +- * Sort (9)\n",
    "               +- Exchange (8)\n",
    "                  +- * Filter (7)\n",
    "                     +- * Scan ExistingRDD (6)\n",
    "```\n",
    "\n",
    "\n",
    "  \n",
    "![Skew Stage Graph](screenshots/01_AQE_disabled_data_skew.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a9d62c-2798-4555-9f89-0d0b86637b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RUNNING WITH AQE ENABLED ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Duration (AQE ON): 7.40 seconds\n",
      "üëâ Go check http://localhost:4040. Look for Coalesced partitions (fewer than 200 tasks).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Cell 4: RUN 2 - AQE ENABLED (The \"Optimized\" Way) Run this after the previous one finishes.\n",
    "\n",
    "print(\"--- RUNNING WITH AQE ENABLED ---\")\n",
    "\n",
    "# Enable AQE Features\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# We keep the \"bad\" static defaults to prove AQE fixes them at runtime\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# The Exact Same Query\n",
    "result = (\n",
    "    df_sales\n",
    "    .join(df_countries, \"country_code\")\n",
    "    .groupBy(\"country_name\")\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "# Trigger Action\n",
    "result.collect()\n",
    "\n",
    "print(f\"‚è±Ô∏è Duration (AQE ON): {time.time() - start:.2f} seconds\")\n",
    "print(\"üëâ Go check http://localhost:4040. Look for Coalesced partitions (fewer than 200 tasks).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bffef-b0ba-468b-aec6-0e13d13d371e",
   "metadata": {},
   "source": [
    "### Observation: Data Skew\n",
    "Notice the tasks have 1/2 partitions even though we started with 200 partitions.\n",
    "Since we do not need 200 partitions, the AQE judged that and reduced the partitions.\n",
    "![Skew Stage Graph](screenshots/02_AQE_enabled_so_1_writing_partition.jpg)\n",
    "\n",
    "Below is the physical plan generated, and the overridden plan that was generated via AQE\n",
    "\n",
    "\n",
    "```== Physical Plan ==\n",
    "AdaptiveSparkPlan (31)\n",
    "+- == Final Plan ==\n",
    "   * HashAggregate (20)\n",
    "   +- AQEShuffleRead (19)\n",
    "      +- ShuffleQueryStage (18), Statistics(sizeInBytes=152.0 B, rowCount=4)\n",
    "         +- Exchange (17)\n",
    "            +- * HashAggregate (16)\n",
    "               +- * Project (15)\n",
    "                  +- * SortMergeJoin Inner (14)\n",
    "                     :- * Sort (7)\n",
    "                     :  +- AQEShuffleRead (6)\n",
    "                     :     +- ShuffleQueryStage (5), Statistics(sizeInBytes=305.2 MiB, rowCount=1.00E+7)\n",
    "                     :        +- Exchange (4)\n",
    "                     :           +- * Filter (3)\n",
    "                     :              +- * ColumnarToRow (2)\n",
    "                     :                 +- Scan parquet  (1)\n",
    "                     +- * Sort (13)\n",
    "                        +- AQEShuffleRead (12)\n",
    "                           +- ShuffleQueryStage (11), Statistics(sizeInBytes=184.0 B, rowCount=4)\n",
    "                              +- Exchange (10)\n",
    "                                 +- * Filter (9)\n",
    "                                    +- * Scan ExistingRDD (8)\n",
    "+- == Initial Plan ==\n",
    "   HashAggregate (30)\n",
    "   +- Exchange (29)\n",
    "      +- HashAggregate (28)\n",
    "         +- Project (27)\n",
    "            +- SortMergeJoin Inner (26)\n",
    "               :- Sort (23)\n",
    "               :  +- Exchange (22)\n",
    "               :     +- Filter (21)\n",
    "               :        +- Scan parquet  (1)\n",
    "               +- Sort (25)\n",
    "                  +- Exchange (24)\n",
    "                     +- Filter (9)\n",
    "                        +- Scan ExistingRDD (8)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5767cf8-84d0-4e5b-bf93-35ae549ea592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
